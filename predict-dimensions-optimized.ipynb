{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-14T07:37:00.276398Z","iopub.execute_input":"2021-10-14T07:37:00.276840Z","iopub.status.idle":"2021-10-14T07:37:00.301111Z","shell.execute_reply.started":"2021-10-14T07:37:00.276805Z","shell.execute_reply":"2021-10-14T07:37:00.299864Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"adversity = pd.read_csv('/kaggle/input/predict-dimensions/adversity_with_features.csv', delimiter = ';')\nnegativity = pd.read_csv('/kaggle/input/predict-dimensions/negativity_with_features.csv', delimiter = ';')\nsociality = pd.read_csv('/kaggle/input/predict-dimensions/sociality_with_features.csv', delimiter = ';')\ndeception = pd.read_csv('/kaggle/input/predict-dimensions/deception_with_features.csv', delimiter = ';')\npositivity = pd.read_csv('/kaggle/input/predict-dimensions/positivity_with_features.csv', delimiter = ';')\nintellect = pd.read_csv('/kaggle/input/predict-dimensions/intellect_with_features.csv', delimiter = ';')\nmating = pd.read_csv('/kaggle/input/predict-dimensions/mating_with_features.csv', delimiter = ';')\nduty = pd.read_csv('/kaggle/input/duty-prediction/duty_with_features.csv', delimiter = ';')\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-10-14T07:37:00.303277Z","iopub.execute_input":"2021-10-14T07:37:00.303866Z","iopub.status.idle":"2021-10-14T07:37:00.439447Z","shell.execute_reply.started":"2021-10-14T07:37:00.303824Z","shell.execute_reply":"2021-10-14T07:37:00.438359Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#run the script separately for each psychological characteristics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Separate target from predictors\n\n#### Adapt this snipet for each psychological characteristic.\n#### E.g. for duty change it to y=duty.Duty X=duty.drop('Duty', axis=1)\ny = adversity.Adversity\nX = adversity.drop('Adversity', axis=1)\n\n\n# Divide data into training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 12 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\n\nX_valid = X_valid_full[my_cols].copy()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:37:05.146053Z","iopub.execute_input":"2021-10-14T07:37:05.146491Z","iopub.status.idle":"2021-10-14T07:37:06.184405Z","shell.execute_reply.started":"2021-10-14T07:37:05.146454Z","shell.execute_reply":"2021-10-14T07:37:06.183293Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"features = ['setting', 'frequency', 'initiator', 'help', 'other_gender', 'role',\n       'hierarchy', 'geo_distance', 'years_known', 'age_difference',\n       'same_gender', 'other_age', 'rel_quality', 'depth_acquaintance',\n       'contact_freq', 'shared_interests', 'formality_level']","metadata":{"execution":{"iopub.status.busy":"2021-08-18T07:56:34.111837Z","iopub.execute_input":"2021-08-18T07:56:34.112249Z","iopub.status.idle":"2021-08-18T07:56:34.118055Z","shell.execute_reply.started":"2021-08-18T07:56:34.112215Z","shell.execute_reply":"2021-08-18T07:56:34.116753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Make copy to avoid changing original data \nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in categorical_cols:\n    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n    label_X_valid[col] = label_encoder.transform(X_valid[col])","metadata":{"execution":{"iopub.status.busy":"2021-08-18T08:00:34.380753Z","iopub.execute_input":"2021-08-18T08:00:34.381201Z","iopub.status.idle":"2021-08-18T08:00:34.409099Z","shell.execute_reply.started":"2021-08-18T08:00:34.381163Z","shell.execute_reply":"2021-08-18T08:00:34.407758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Imputation for label\nmy_imputer = SimpleImputer()\nlabel_imputed_X_train = pd.DataFrame(my_imputer.fit_transform(label_X_train))\nlabel_imputed_X_valid = pd.DataFrame(my_imputer.transform(label_X_valid))\n\n# Imputation removed column names; put them back\nlabel_imputed_X_train.columns = label_X_train.columns\nlabel_imputed_X_valid.columns = label_X_valid.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-18T08:01:25.398124Z","iopub.execute_input":"2021-08-18T08:01:25.398499Z","iopub.status.idle":"2021-08-18T08:01:25.41338Z","shell.execute_reply.started":"2021-08-18T08:01:25.398467Z","shell.execute_reply":"2021-08-18T08:01:25.412307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = label_imputed_X_train\nX_valid = label_imputed_X_valid","metadata":{"execution":{"iopub.status.busy":"2021-08-18T08:00:38.831107Z","iopub.execute_input":"2021-08-18T08:00:38.831535Z","iopub.status.idle":"2021-08-18T08:00:38.836633Z","shell.execute_reply.started":"2021-08-18T08:00:38.831499Z","shell.execute_reply":"2021-08-18T08:00:38.835233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state = 42)\nfrom pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:02:00.103755Z","iopub.execute_input":"2021-08-17T17:02:00.104339Z","iopub.status.idle":"2021-08-17T17:02:00.115637Z","shell.execute_reply.started":"2021-08-17T17:02:00.104303Z","shell.execute_reply":"2021-08-17T17:02:00.114749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\npprint(random_grid)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:02:03.214669Z","iopub.execute_input":"2021-08-17T17:02:03.215015Z","iopub.status.idle":"2021-08-17T17:02:03.226052Z","shell.execute_reply.started":"2021-08-17T17:02:03.214987Z","shell.execute_reply":"2021-08-17T17:02:03.225375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:09:16.485348Z","iopub.execute_input":"2021-08-17T17:09:16.485731Z","iopub.status.idle":"2021-08-17T17:17:57.33395Z","shell.execute_reply.started":"2021-08-17T17:09:16.485697Z","shell.execute_reply":"2021-08-17T17:17:57.332937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_random.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:22:28.641702Z","iopub.execute_input":"2021-08-17T17:22:28.642134Z","iopub.status.idle":"2021-08-17T17:22:28.648612Z","shell.execute_reply.started":"2021-08-17T17:22:28.642099Z","shell.execute_reply":"2021-08-17T17:22:28.647586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nmodel = rf_random.best_estimator_\npredictions = model.predict(X_valid)\n\n\nmae = mean_absolute_error(predictions, y_valid)\nmse = mean_squared_error(predictions, y_valid, squared=False)\n\nprint(mae)\nprint(mse)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T08:01:32.34431Z","iopub.execute_input":"2021-08-18T08:01:32.344684Z","iopub.status.idle":"2021-08-18T08:01:32.353065Z","shell.execute_reply.started":"2021-08-18T08:01:32.344653Z","shell.execute_reply":"2021-08-18T08:01:32.351939Z"},"trusted":true},"execution_count":null,"outputs":[]}]}