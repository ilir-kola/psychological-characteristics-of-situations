{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-14T07:51:27.301609Z","iopub.execute_input":"2021-10-14T07:51:27.301927Z","iopub.status.idle":"2021-10-14T07:51:28.193012Z","shell.execute_reply.started":"2021-10-14T07:51:27.301897Z","shell.execute_reply":"2021-10-14T07:51:28.192125Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"priority_data = pd.read_csv('/kaggle/input/dimensions-priority/dimensions_priority.csv', delimiter = ';')\npriority_data.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-10-14T07:51:44.352314Z","iopub.execute_input":"2021-10-14T07:51:44.352636Z","iopub.status.idle":"2021-10-14T07:51:44.404533Z","shell.execute_reply.started":"2021-10-14T07:51:44.352607Z","shell.execute_reply":"2021-10-14T07:51:44.403373Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Separate target from predictors\ny = priority_data.Priority\nX = priority_data.drop('Priority', axis=1)\n\n\n# Divide data into training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 12 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\n\nX_valid = X_valid_full[my_cols].copy()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:52:55.436454Z","iopub.execute_input":"2021-10-14T07:52:55.436842Z","iopub.status.idle":"2021-10-14T07:52:56.620481Z","shell.execute_reply.started":"2021-10-14T07:52:55.436805Z","shell.execute_reply":"2021-10-14T07:52:56.619427Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"features = ['Duty', 'Intellect', 'Adversity', 'Mating', 'Positivity', 'Negativity', 'Deception', 'sociality']","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:53:05.496399Z","iopub.execute_input":"2021-10-14T07:53:05.496729Z","iopub.status.idle":"2021-10-14T07:53:05.501877Z","shell.execute_reply.started":"2021-10-14T07:53:05.496698Z","shell.execute_reply":"2021-10-14T07:53:05.500781Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Make copy to avoid changing original data \nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in categorical_cols:\n    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n    label_X_valid[col] = label_encoder.transform(X_valid[col])","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:54:24.062177Z","iopub.execute_input":"2021-10-14T07:54:24.062494Z","iopub.status.idle":"2021-10-14T07:54:24.069671Z","shell.execute_reply.started":"2021-10-14T07:54:24.062452Z","shell.execute_reply":"2021-10-14T07:54:24.068577Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Imputation for label\nmy_imputer = SimpleImputer()\nlabel_imputed_X_train = pd.DataFrame(my_imputer.fit_transform(label_X_train))\nlabel_imputed_X_valid = pd.DataFrame(my_imputer.transform(label_X_valid))\n\n# Imputation removed column names; put them back\nlabel_imputed_X_train.columns = label_X_train.columns\nlabel_imputed_X_valid.columns = label_X_valid.columns","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:54:26.470791Z","iopub.execute_input":"2021-10-14T07:54:26.471093Z","iopub.status.idle":"2021-10-14T07:54:26.618863Z","shell.execute_reply.started":"2021-10-14T07:54:26.471066Z","shell.execute_reply":"2021-10-14T07:54:26.618006Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X_train = label_imputed_X_train\nX_valid = label_imputed_X_valid","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:54:29.059759Z","iopub.execute_input":"2021-10-14T07:54:29.060094Z","iopub.status.idle":"2021-10-14T07:54:29.065102Z","shell.execute_reply.started":"2021-10-14T07:54:29.060061Z","shell.execute_reply":"2021-10-14T07:54:29.064018Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state = 42)\nfrom pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:55:53.558161Z","iopub.execute_input":"2021-10-14T07:55:53.558531Z","iopub.status.idle":"2021-10-14T07:55:53.628512Z","shell.execute_reply.started":"2021-10-14T07:55:53.558496Z","shell.execute_reply":"2021-10-14T07:55:53.627727Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\npprint(random_grid)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:55:56.989115Z","iopub.execute_input":"2021-10-14T07:55:56.989415Z","iopub.status.idle":"2021-10-14T07:55:57.001599Z","shell.execute_reply.started":"2021-10-14T07:55:56.989387Z","shell.execute_reply":"2021-10-14T07:55:57.000433Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:56:02.170229Z","iopub.execute_input":"2021-10-14T07:56:02.170671Z","iopub.status.idle":"2021-10-14T08:02:15.648157Z","shell.execute_reply.started":"2021-10-14T07:56:02.170622Z","shell.execute_reply":"2021-10-14T08:02:15.647230Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"rf_random.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-10-14T08:02:19.533289Z","iopub.execute_input":"2021-10-14T08:02:19.533667Z","iopub.status.idle":"2021-10-14T08:02:19.540572Z","shell.execute_reply.started":"2021-10-14T08:02:19.533624Z","shell.execute_reply":"2021-10-14T08:02:19.539744Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = rf_random.best_estimator_\npredictions = model.predict(X_valid)\nerrors = abs(predictions - y_valid)\nmape = 100 * np.mean(errors / y_valid)\naccuracy = 100 - mape\nprint('Model Performance')\nprint('Mean Average Error: {:0.4f}.'.format(np.mean(errors)))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T08:02:35.768858Z","iopub.execute_input":"2021-10-14T08:02:35.769374Z","iopub.status.idle":"2021-10-14T08:02:36.160719Z","shell.execute_reply.started":"2021-10-14T08:02:35.769323Z","shell.execute_reply":"2021-10-14T08:02:36.159633Z"},"trusted":true},"execution_count":13,"outputs":[]}]}